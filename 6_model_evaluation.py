# ==============================================
# 6_model_evaluation.py
# åŠŸèƒ½ï¼šç”¨æµ‹è¯•é›†è¯„ä¼°ä¿å­˜çš„.pthæ¨¡åž‹ï¼Œè¾“å‡ºå‡†ç¡®çŽ‡ã€æ··æ·†çŸ©é˜µã€åˆ†ç±»æŠ¥å‘Š
# ä¾èµ–ï¼šéœ€å…ˆè¿è¡Œ5_model_training.pyï¼Œç”Ÿæˆfinal_model_1epoch.pth
# ==============================================

# æ­¥éª¤1ï¼šå¯¼å…¥ä¾èµ–åº“
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import glob
import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns  # ç”¨äºŽç»˜åˆ¶æ··æ·†çŸ©é˜µï¼ˆè‹¥æœªå®‰è£…ï¼Œå…ˆè¿è¡Œï¼špip install seabornï¼‰

# æ­¥éª¤2ï¼šå¤ç”¨å¿…è¦çš„ç±»ï¼ˆæ•°æ®é›†ç±»+æ¨¡åž‹ç±»ï¼Œç¡®ä¿èƒ½åŠ è½½.pthæ¨¡åž‹ï¼‰
# ---------------------- æ•°æ®é›†ç±»ï¼ˆä¸Žè®­ç»ƒä»£ç ä¸€è‡´ï¼‰ ----------------------
class EmoreFaceDataset(Dataset):
    def __init__(self, img_dir, transform=None):
        self.img_dir = img_dir
        self.transform = transform
        self.img_paths = glob.glob(os.path.join(img_dir, "*/*.jpg")) + glob.glob(os.path.join(img_dir, "*/*.png"))
        self.class_names = sorted(os.listdir(img_dir))  # ç±»åˆ«åç§°ï¼ˆå¦‚['class0', 'class1']ï¼‰
        self.class_to_idx = {cls: i for i, cls in enumerate(self.class_names)}
        self.labels = [self.class_to_idx[os.path.basename(os.path.dirname(path))] for path in self.img_paths]

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):
        img_path = self.img_paths[idx]
        label = self.labels[idx]
        image = Image.open(img_path).convert("RGB")
        if self.transform:
            image = self.transform(image)
        return image, label, img_path  # é¢å¤–è¿”å›žå›¾ç‰‡è·¯å¾„ï¼Œä¾¿äºŽåŽç»­åˆ†æžé”™è¯¯æ ·æœ¬

# ---------------------- æ¨¡åž‹ç±»ï¼ˆä¸Žè®­ç»ƒä»£ç ä¸€è‡´ï¼‰ ----------------------
class PyTorch_Native_CNN(nn.Module):
    def __init__(self, num_classes=2):
        super(PyTorch_Native_CNN, self).__init__()
        self.conv_layers = nn.Sequential(
            nn.Conv2d(3, 32, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Dropout(0.25),
            nn.Conv2d(32, 64, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Dropout(0.25),
            nn.Conv2d(64, 128, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Dropout(0.25)
        )
        self.fc_layers = nn.Sequential(
            nn.Linear(128 * 14 * 14, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        x = self.conv_layers(x)
        x = x.view(-1, 128 * 14 * 14)
        x = self.fc_layers(x)
        return x

# æ­¥éª¤3ï¼šé…ç½®è¯„ä¼°å‚æ•°
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
TEST_DATA_DIR = "./data/test"  # æµ‹è¯•é›†è·¯å¾„ï¼ˆä¸Žè®­ç»ƒä»£ç ä¸€è‡´ï¼‰
MODEL_PATH = "./checkpoints/final_model_1epoch.pth"  # ä¿å­˜çš„.pthæ¨¡åž‹è·¯å¾„
BATCH_SIZE = 32
NUM_CLASSES = 2  # äºŒåˆ†ç±»ä»»åŠ¡

# æ­¥éª¤4ï¼šåŠ è½½æµ‹è¯•é›†ï¼ˆé¢„å¤„ç†ä¸Žè®­ç»ƒæ—¶ä¸€è‡´ï¼‰
test_transform = transforms.Compose([
    transforms.Resize((112, 112)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

# åŠ è½½æµ‹è¯•é›†
test_dataset = EmoreFaceDataset(img_dir=TEST_DATA_DIR, transform=test_transform)
test_loader = DataLoader(
    test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0
)

# èŽ·å–ç±»åˆ«åç§°ï¼ˆç”¨äºŽæ··æ·†çŸ©é˜µæ ‡æ³¨ï¼‰
class_names = test_dataset.class_names
print("="*60)
print("æ¨¡åž‹è¯„ä¼°å¼€å§‹ï¼")
print(f"è¯„ä¼°è®¾å¤‡ï¼š{DEVICE}")
print(f"æµ‹è¯•é›†æ ·æœ¬æ•°ï¼š{len(test_dataset)}")
print(f"æµ‹è¯•é›†ç±»åˆ«ï¼š{class_names}ï¼ˆå…±{NUM_CLASSES}ç±»ï¼‰")
print(f"åŠ è½½çš„æ¨¡åž‹æ–‡ä»¶ï¼š{MODEL_PATH}")
print("="*60)

# æ­¥éª¤5ï¼šåŠ è½½.pthæ¨¡åž‹
# åˆå§‹åŒ–æ¨¡åž‹
model = PyTorch_Native_CNN(num_classes=NUM_CLASSES).to(DEVICE)
# åŠ è½½.pthæ–‡ä»¶ä¸­çš„æƒé‡å’Œä¿¡æ¯
checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)  # map_locationé€‚é…CPU/GPU
model.load_state_dict(checkpoint["model_state_dict"])  # åŠ è½½æ¨¡åž‹æƒé‡
model.eval()  # è®¾ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆç¦ç”¨Dropoutï¼Œç¡®ä¿é¢„æµ‹ç¨³å®šï¼‰

# æ‰“å°æ¨¡åž‹è®­ç»ƒæ—¶çš„ä¿¡æ¯ï¼ˆå¯¹æ¯”å‚è€ƒï¼‰
print(f"\nã€æ¨¡åž‹è®­ç»ƒä¿¡æ¯ã€‘")
print(f"è®­ç»ƒè½®æ•°ï¼š{checkpoint['epoch']}è½®")
print(f"è®­ç»ƒé›†å‡†ç¡®çŽ‡ï¼š{checkpoint['train_acc']:.4f}")
print(f"éªŒè¯é›†å‡†ç¡®çŽ‡ï¼š{checkpoint['val_acc']:.4f}")
print(f"è®­ç»ƒè¶…å‚æ•°ï¼š{checkpoint['hyper_params']}")
print("-"*60)

# æ­¥éª¤6ï¼šåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡åž‹
all_preds = []  # å­˜å‚¨æ‰€æœ‰é¢„æµ‹æ ‡ç­¾
all_labels = []  # å­˜å‚¨æ‰€æœ‰çœŸå®žæ ‡ç­¾
all_img_paths = []  # å­˜å‚¨æ‰€æœ‰å›¾ç‰‡è·¯å¾„ï¼ˆä¾¿äºŽåˆ†æžé”™è¯¯æ ·æœ¬ï¼‰

with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒåŠ å¿«è¯„ä¼°é€Ÿåº¦
    total_correct = 0
    total_samples = 0

    for batch_idx, (images, labels, img_paths) in enumerate(test_loader):
        # æ•°æ®è¿ç§»åˆ°è®¾å¤‡
        images, labels = images.to(DEVICE), labels.to(DEVICE)
        # æ¨¡åž‹é¢„æµ‹
        outputs = model(images)
        _, preds = torch.max(outputs.data, 1)  # å–æ¦‚çŽ‡æœ€å¤§çš„ç±»åˆ«ä½œä¸ºé¢„æµ‹ç»“æžœ

        # ç´¯è®¡ç»Ÿè®¡
        total_samples += labels.size(0)
        total_correct += (preds == labels).sum().item()

        # ä¿å­˜é¢„æµ‹ç»“æžœã€çœŸå®žæ ‡ç­¾ã€å›¾ç‰‡è·¯å¾„ï¼ˆç”¨äºŽåŽç»­è®¡ç®—æ··æ·†çŸ©é˜µï¼‰
        all_preds.extend(preds.cpu().numpy())  # è½¬ç§»åˆ°CPUå¹¶è½¬ä¸ºnumpy
        all_labels.extend(labels.cpu().numpy())
        all_img_paths.extend(img_paths)

        # æ‰“å°è¯„ä¼°è¿›åº¦
        if (batch_idx + 1) % 20 == 0:
            batch_acc = (preds == labels).sum().item() / labels.size(0)
            print(f"  æ‰¹æ¬¡[{batch_idx+1}/{len(test_loader)}] - æ‰¹æ¬¡å‡†ç¡®çŽ‡: {batch_acc:.4f}")

# æ­¥éª¤7ï¼šè®¡ç®—æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡
test_acc = total_correct / total_samples  # æµ‹è¯•é›†æ€»ä½“å‡†ç¡®çŽ‡
# è®¡ç®—æ··æ·†çŸ©é˜µï¼ˆsklearnå®žçŽ°ï¼‰
cm = confusion_matrix(all_labels, all_preds)
# è®¡ç®—åˆ†ç±»æŠ¥å‘Šï¼ˆç²¾ç¡®çŽ‡ã€å¬å›žçŽ‡ã€F1åˆ†æ•°ï¼‰
class_report = classification_report(
    all_labels, all_preds, target_names=class_names, output_dict=True
)

# æ­¥éª¤8ï¼šæ‰“å°è¯„ä¼°ç»“æžœ
print("\n" + "="*60)
print("ã€æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°ç»“æžœã€‘")
print("="*60)
print(f"æµ‹è¯•é›†æ€»ä½“å‡†ç¡®çŽ‡ï¼š{test_acc:.4f}")
print("\nã€åˆ†ç±»è¯¦ç»†æŒ‡æ ‡ã€‘")
for i, cls in enumerate(class_names):
    precision = class_report[cls]["precision"]
    recall = class_report[cls]["recall"]
    f1_score = class_report[cls]["f1-score"]
    support = class_report[cls]["support"]
    print(f"{cls} - ç²¾ç¡®çŽ‡: {precision:.4f} | å¬å›žçŽ‡: {recall:.4f} | F1åˆ†æ•°: {f1_score:.4f} | æ ·æœ¬æ•°: {support}")

print("\nã€æ··æ·†çŸ©é˜µã€‘")
print(cm)
print("="*60)

# æ­¥éª¤9ï¼šå¯è§†åŒ–æ··æ·†çŸ©é˜µï¼ˆç›´è§‚å±•ç¤ºåˆ†ç±»æ•ˆæžœï¼‰
plt.figure(figsize=(8, 6))
sns.heatmap(
    cm, annot=True, fmt="d", cmap="Blues",
    xticklabels=class_names,
    yticklabels=class_names
)
plt.xlabel("é¢„æµ‹æ ‡ç­¾")
plt.ylabel("çœŸå®žæ ‡ç­¾")
plt.title(f"æ¨¡åž‹æ··æ·†çŸ©é˜µï¼ˆæµ‹è¯•é›†å‡†ç¡®çŽ‡ï¼š{test_acc:.4f}ï¼‰")
plt.tight_layout()
# ä¿å­˜æ··æ·†çŸ©é˜µå›¾ç‰‡åˆ°é¡¹ç›®æ ¹ç›®å½•
plt.savefig("confusion_matrix.png", dpi=150)
plt.show()
print(f"\nâœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜ä¸ºï¼šconfusion_matrix.pngï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰")

# æ­¥éª¤10ï¼šåˆ†æžé”™è¯¯æ ·æœ¬ï¼ˆå¯é€‰ï¼Œå¸®åŠ©ä¼˜åŒ–æ¨¡åž‹ï¼‰
print("\nã€é”™è¯¯æ ·æœ¬åˆ†æžï¼ˆå‰5ä¸ªï¼‰ã€‘")
print("-"*60)
error_count = 0
for img_path, true_label, pred_label in zip(all_img_paths, all_labels, all_preds):
    if true_label != pred_label:
        true_cls = class_names[true_label]
        pred_cls = class_names[pred_label]
        print(f"é”™è¯¯æ ·æœ¬ï¼š{img_path}")
        print(f"  çœŸå®žç±»åˆ«ï¼š{true_cls} | é¢„æµ‹ç±»åˆ«ï¼š{pred_cls}")
        error_count += 1
        if error_count >= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯æ ·æœ¬
            break

if error_count == 0:
    print("ðŸŽ‰ æ— é”™è¯¯æ ·æœ¬ï¼æ‰€æœ‰æµ‹è¯•é›†æ ·æœ¬åˆ†ç±»æ­£ç¡®ï½ž")
print("="*60)

# æ­¥éª¤11ï¼šå®žéªŒæ€»ç»“
print("\nã€å®žéªŒæ€»ç»“ã€‘")
print("="*60)
print(f"1. æ¨¡åž‹è®­ç»ƒï¼š1è½®è®­ç»ƒï¼Œæ€»è€—æ—¶{checkpoint['hyper_params'].get('train_time', 'N/A')}åˆ†é’Ÿ")
print(f"2. æ¨¡åž‹æ€§èƒ½ï¼šæµ‹è¯•é›†å‡†ç¡®çŽ‡{test_acc:.4f}ï¼ŒéªŒè¯é›†å‡†ç¡®çŽ‡{checkpoint['val_acc']:.4f}")
print(f"3. æ¨¡åž‹æ–‡ä»¶ï¼š{MODEL_PATH}ï¼ˆå¯ç›´æŽ¥ç”¨äºŽåŽç»­é¢„æµ‹ï¼‰")
print(f"4. å¯è§†åŒ–ç»“æžœï¼šconfusion_matrix.pngï¼ˆæ··æ·†çŸ©é˜µå›¾ç‰‡ï¼‰")
print(f"5. å®žéªŒç»“è®ºï¼š1è½®è®­ç»ƒå·²è¾¾åˆ°è‰¯å¥½åˆ†ç±»æ•ˆæžœï¼ˆ>90%å‡†ç¡®çŽ‡ï¼‰ï¼Œæ»¡è¶³å®žéªŒéœ€æ±‚ï½ž")
print("="*60)